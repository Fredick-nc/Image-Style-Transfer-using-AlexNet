{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41dfed0d",
   "metadata": {},
   "source": [
    "# Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e2654c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yunhuinie/opt/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as functional\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "import cv2\n",
    "import argparse\n",
    "import time\n",
    "import torch\n",
    "import torchvision\n",
    "import os\n",
    "import random\n",
    "import torch.nn.functional as functional"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1932675",
   "metadata": {},
   "source": [
    "# Content loss："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74fa9937",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContentLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    内容损失层\n",
    "    \"\"\"\n",
    "    def __init__(self, target):\n",
    "        super(ContentLoss, self).__init__()\n",
    "        self.target = target.detach()\n",
    "        self.loss = None\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.loss = functional.mse_loss(input, self.target)\n",
    "        return input\n",
    "\n",
    "    def update(self, target):\n",
    "        \"\"\"\n",
    "        内容损失的更新函数\n",
    "        \"\"\"\n",
    "        self.target = target.detach()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47476bcb",
   "metadata": {},
   "source": [
    "# Style loss："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8bf6f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StyleLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    风格损失层\n",
    "    \"\"\"\n",
    "    def __init__(self, target, patch_size, mrf_style_stride, mrf_synthesis_stride, gpu_chunck_size, device):\n",
    "        super(StyleLoss, self).__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.mrf_style_stride = mrf_style_stride\n",
    "        self.mrf_synthesis_stride = mrf_synthesis_stride\n",
    "        self.gpu_chunck_size = gpu_chunck_size\n",
    "        self.device = device\n",
    "        self.loss = None\n",
    "\n",
    "        self.style_patches = self.patches_sampling(target.detach(), patch_size=self.patch_size, stride=self.mrf_style_stride)\n",
    "        self.style_patches_norm = self.cal_patches_norm()\n",
    "        self.style_patches_norm = self.style_patches_norm.view(-1, 1, 1)\n",
    "\n",
    "    def update(self, target):\n",
    "        \"\"\"\n",
    "        更新风格损失的目标函数\n",
    "        \"\"\"\n",
    "        self.style_patches = self.patches_sampling(target.detach(), patch_size=self.patch_size,\n",
    "                                                   stride=self.mrf_style_stride)\n",
    "        self.style_patches_norm = self.cal_patches_norm()\n",
    "        self.style_patches_norm = self.style_patches_norm.view(-1, 1, 1)\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        计算马尔可夫损失，输入参数为合成图像\n",
    "        \"\"\"\n",
    "        synthesis_patches = self.patches_sampling(input, patch_size=self.patch_size, stride=self.mrf_synthesis_stride)\n",
    "        max_response = []\n",
    "        for i in range(0, self.style_patches.shape[0], self.gpu_chunck_size):\n",
    "            i_start = i\n",
    "            i_end = min(i+self.gpu_chunck_size, self.style_patches.shape[0])\n",
    "            weight = self.style_patches[i_start:i_end, :, :, :]\n",
    "            response = functional.conv2d(input, weight, stride=self.mrf_synthesis_stride)\n",
    "            max_response.append(response.squeeze(dim=0))\n",
    "        max_response = torch.cat(max_response, dim=0)\n",
    "\n",
    "        max_response = max_response.div(self.style_patches_norm)\n",
    "        max_response = torch.argmax(max_response, dim=0)\n",
    "        max_response = torch.reshape(max_response, (1, -1)).squeeze()\n",
    "        # 损失\n",
    "        loss = 0\n",
    "        for i in range(0, len(max_response), self.gpu_chunck_size):\n",
    "            i_start = i\n",
    "            i_end = min(i+self.gpu_chunck_size, len(max_response))\n",
    "            tp_ind = tuple(range(i_start, i_end))\n",
    "            sp_ind = max_response[i_start:i_end]\n",
    "            loss += torch.sum(torch.mean(torch.pow(synthesis_patches[tp_ind, :, :, :]-self.style_patches[sp_ind, :, :, :], 2), dim=[1, 2, 3]))\n",
    "        self.loss = loss/len(max_response)\n",
    "        return input\n",
    "\n",
    "    def patches_sampling(self, image, patch_size, stride):\n",
    "        \"\"\"\n",
    "        对块采样形成一个图片\n",
    "        第二个参数为图像\n",
    "        第三个参数为块的大小\n",
    "        \"\"\"\n",
    "        h, w = image.shape[2:4]\n",
    "        patches = []\n",
    "        for i in range(0, h - patch_size + 1, stride):\n",
    "            for j in range(0, w - patch_size + 1, stride):\n",
    "                patches.append(image[:, :, i:i + patch_size, j:j + patch_size])\n",
    "        patches = torch.cat(patches, dim=0).to(self.device)\n",
    "        return patches\n",
    "\n",
    "    def cal_patches_norm(self):\n",
    "        \"\"\"\n",
    "        计算风格图片块的标准矩阵\n",
    "        \"\"\"\n",
    "        norm_array = torch.zeros(self.style_patches.shape[0])\n",
    "        for i in range(self.style_patches.shape[0]):\n",
    "            norm_array[i] = torch.pow(torch.sum(torch.pow(self.style_patches[i], 2)), 0.5)\n",
    "        return norm_array.to(self.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1de7670f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TVLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        总变差损失层\n",
    "        \"\"\"\n",
    "        super(TVLoss, self).__init__()\n",
    "        self.loss = None\n",
    "\n",
    "    def forward(self, input):\n",
    "        image = input.squeeze().permute([1, 2, 0])\n",
    "        r = (image[:, :, 0] + 2.12) / 4.37\n",
    "        g = (image[:, :, 1] + 2.04) / 4.46\n",
    "        b = (image[:, :, 2] + 1.80) / 4.44\n",
    "\n",
    "        temp = torch.cat([r.unsqueeze(2), g.unsqueeze(2), b.unsqueeze(2)], dim=2)\n",
    "        gx = torch.cat((temp[1:, :, :], temp[-1, :, :].unsqueeze(0)), dim=0)\n",
    "        gx = gx - temp\n",
    "\n",
    "        gy = torch.cat((temp[:, 1:, :], temp[:, -1, :].unsqueeze(1)), dim=1)\n",
    "        gy = gy - temp\n",
    "\n",
    "        self.loss = torch.mean(torch.pow(gx, 2)) + torch.mean(torch.pow(gy, 2))\n",
    "        return input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47666395",
   "metadata": {},
   "source": [
    "# Convolution layer of Markov random matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0711a9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNMRF(nn.Module):\n",
    "    def __init__(self, style_image, content_image, device, content_weight, style_weight, tv_weight, gpu_chunck_size=256, mrf_style_stride=2,\n",
    "                 mrf_synthesis_stride=2):\n",
    "        super(CNNMRF, self).__init__()\n",
    "        # 在内容和风格之间调解权重\n",
    "        self.content_weight = content_weight\n",
    "        self.style_weight = style_weight\n",
    "        self.tv_weight = tv_weight\n",
    "        self.patch_size = 3\n",
    "        self.device = device\n",
    "        self.gpu_chunck_size = gpu_chunck_size\n",
    "        self.mrf_style_stride = mrf_style_stride\n",
    "        self.mrf_synthesis_stride = mrf_synthesis_stride\n",
    "        self.style_layers = [11, 20]\n",
    "        self.content_layers = [22]\n",
    "        self.model, self.content_losses, self.style_losses, self.tv_loss = \\\n",
    "            self.get_model_and_losses(style_image=style_image, content_image=content_image)\n",
    "\n",
    "    def forward(self, synthesis):\n",
    "        \"\"\"\n",
    "        计算损失并返回新的损失\n",
    "        参数为合成的图像\n",
    "        \"\"\"\n",
    "        self.model(synthesis)\n",
    "        style_score = 0\n",
    "        content_score = 0\n",
    "        tv_score = self.tv_loss.loss\n",
    "\n",
    "        # 计算风格损失\n",
    "        for sl in self.style_losses:\n",
    "            style_score += sl.loss\n",
    "\n",
    "        # 计算内容损失\n",
    "        for cl in self.content_losses:\n",
    "            content_score += cl.loss\n",
    "\n",
    "        # 计算最后的损失\n",
    "        loss = self.style_weight * style_score + self.content_weight * content_score + self.tv_weight * tv_score\n",
    "        return loss\n",
    "\n",
    "    def update_style_and_content_image(self, style_image, content_image):\n",
    "        \"\"\"\n",
    "        更新内容的目标损失层和风格的目标损失层\n",
    "        第一个参数为风格图片\n",
    "        第二个参数为内容图片\n",
    "        \"\"\"\n",
    "        # 更新风格目标损失层\n",
    "        x = style_image.clone()\n",
    "        next_style_idx = 0\n",
    "        i = 0\n",
    "        for layer in self.model:\n",
    "            if isinstance(layer, TVLoss) or isinstance(layer, ContentLoss) or isinstance(layer, StyleLoss):\n",
    "                continue\n",
    "            if next_style_idx >= len(self.style_losses):\n",
    "                break\n",
    "            x = layer(x)\n",
    "            if i in self.style_layers:\n",
    "                # 在vgg模型中提取风格特征作为风格损失目标\n",
    "                self.style_losses[next_style_idx].update(x)\n",
    "                next_style_idx += 1\n",
    "            i += 1\n",
    "\n",
    "        # 更新内容损失层目标函数\n",
    "        x = content_image.clone()\n",
    "        next_content_idx = 0\n",
    "        i = 0\n",
    "        for layer in self.model:\n",
    "            if isinstance(layer, TVLoss) or isinstance(layer, ContentLoss) or isinstance(layer, StyleLoss):\n",
    "                continue\n",
    "            if next_content_idx >= len(self.content_losses):\n",
    "                break\n",
    "            x = layer(x)\n",
    "            if i in self.content_layers:\n",
    "                # 在vgg模型中提取内容特征作为内容损失目标\n",
    "                self.content_losses[next_content_idx].update(x)\n",
    "                next_content_idx += 1\n",
    "            i += 1\n",
    "\n",
    "    def get_model_and_losses(self, style_image, content_image):\n",
    "        \"\"\"\n",
    "        通过插入vgg19和自定义的损失函数层创建网络模型\n",
    "        第一个参数为风格图片\n",
    "        第二个参数为内容图片\n",
    "        \"\"\"\n",
    "        vgg = models.vgg19(pretrained=True).to(self.device)\n",
    "        model = nn.Sequential()\n",
    "        content_losses = []\n",
    "        style_losses = []\n",
    "        # 添加总变差损失层\n",
    "        tv_loss = TVLoss()\n",
    "        model.add_module('tv_loss', tv_loss)\n",
    "\n",
    "        next_content_idx = 0\n",
    "        next_style_idx = 0\n",
    "\n",
    "        for i in range(len(vgg.features)):\n",
    "            if next_content_idx >= len(self.content_layers) and next_style_idx >= len(self.style_layers):\n",
    "                break\n",
    "            # 加入vgg19模型\n",
    "            layer = vgg.features[i]\n",
    "            name = str(i)\n",
    "            model.add_module(name, layer)\n",
    "\n",
    "            # 加入内容损失层\n",
    "            if i in self.content_layers:\n",
    "                target = model(content_image).detach()\n",
    "                content_loss = ContentLoss(target)\n",
    "                model.add_module(\"content_loss_{}\".format(next_content_idx), content_loss)\n",
    "                content_losses.append(content_loss)\n",
    "                next_content_idx += 1\n",
    "\n",
    "            # 加入风格损失层\n",
    "            if i in self.style_layers:\n",
    "                target_feature = model(style_image).detach()\n",
    "                style_loss = StyleLoss(target_feature, patch_size=self.patch_size, mrf_style_stride=self.mrf_style_stride,\n",
    "                                       mrf_synthesis_stride=self.mrf_synthesis_stride, gpu_chunck_size=self.gpu_chunck_size, device=self.device)\n",
    "\n",
    "                model.add_module(\"style_loss_{}\".format(next_style_idx), style_loss)\n",
    "                style_losses.append(style_loss)\n",
    "                next_style_idx += 1\n",
    "\n",
    "        return model, content_losses, style_losses, tv_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "77c0e8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_synthesis_image(synthesis, denorm, device):\n",
    "    \"\"\"\n",
    "    获取合成图片，去规范化：从tensor转为numpy矩阵形式\n",
    "    第一个参数为合成图像的tensor形式\n",
    "    第二个参数是去规范化的方法\n",
    "    \"\"\"\n",
    "    cpu_device = torch.device('cpu')\n",
    "\n",
    "    image = synthesis.clone().squeeze().to(cpu_device)\n",
    "\n",
    "    image = denorm(image)  #图片去规范化，还原\n",
    "\n",
    "    return image.to(device).clamp_(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad30e3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unsample_synthesis(height, width, synthesis, device):\n",
    "    \"\"\"\n",
    "    对合成图片进行下采样\n",
    "    第一个参数为下采样图像的高度\n",
    "    第二个参数为下采样图像的宽度\n",
    "    第三个参数为合成图像准备下采样的tensor形式\n",
    "    第四个参数表示使用的是cpu还是gpu\n",
    "    \"\"\"\n",
    "    # 将tensor转换为numpy形式，并作为图片进行下采样\n",
    "    synthesis = functional.interpolate(synthesis, size=[height, width], mode='bilinear')  #双线值插值方法\n",
    "    synthesis = synthesis.clone().detach().requires_grad_(True).to(device)\n",
    "    return synthesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1911058d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yunhuinie/opt/anaconda3/envs/tensorflow/lib/python3.6/site-packages/torch/nn/functional.py:3635: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  \"See the documentation of nn.Upsample for details.\".format(mode)\n",
      "/Users/yunhuinie/opt/anaconda3/envs/tensorflow/lib/python3.6/site-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res-1-iteration-10: 43.044933\n",
      "res-1-iteration-20: 42.109062\n",
      "res-1-iteration-30: 41.510006\n",
      "res-1-iteration-40: 41.014008\n",
      "res-1-iteration-50: 40.609325\n",
      "save image: res-1-result-50.jpg\n",
      "res-1-iteration-60: 40.344543\n",
      "res-1-iteration-70: 40.220802\n",
      "res-1-iteration-80: 40.288956\n",
      "res-1-iteration-90: 40.091732\n",
      "res-1-iteration-100: 40.072079\n",
      "save image: res-1-result-100.jpg\n",
      "res-1-iteration-110: 39.936611\n",
      "res-1-iteration-120: 39.840050\n",
      "res-1-iteration-130: 39.803291\n",
      "res-1-iteration-140: 39.722916\n",
      "res-1-iteration-150: 39.675804\n",
      "save image: res-1-result-150.jpg\n",
      "res-1-iteration-160: 39.946712\n",
      "res-1-iteration-170: 39.842201\n",
      "res-1-iteration-180: 39.854626\n",
      "res-1-iteration-190: 39.919727\n",
      "res-1-iteration-200: 39.736038\n",
      "save image: res-1-result-200.jpg\n",
      "res-2-iteration-10: 30.737274\n",
      "res-2-iteration-20: 27.304382\n",
      "res-2-iteration-30: 26.066414\n",
      "res-2-iteration-40: 25.464260\n",
      "res-2-iteration-50: 25.124203\n",
      "save image: res-2-result-50.jpg\n",
      "res-2-iteration-60: 24.881317\n",
      "res-2-iteration-70: 24.708612\n",
      "res-2-iteration-80: 24.590000\n",
      "res-2-iteration-90: 24.482553\n",
      "res-2-iteration-100: 24.394836\n",
      "save image: res-2-result-100.jpg\n",
      "res-2-iteration-110: 24.314474\n",
      "res-2-iteration-120: 24.255140\n",
      "res-2-iteration-130: 24.199036\n",
      "res-2-iteration-140: 24.163055\n",
      "res-2-iteration-150: 24.114681\n",
      "save image: res-2-result-150.jpg\n",
      "res-2-iteration-160: 24.079739\n",
      "res-2-iteration-170: 24.046284\n",
      "res-2-iteration-180: 24.027967\n",
      "res-2-iteration-190: 24.002466\n",
      "res-2-iteration-200: 23.983084\n",
      "save image: res-2-result-200.jpg\n",
      "res-3-iteration-10: 18.297869\n",
      "res-3-iteration-20: 15.973391\n",
      "res-3-iteration-30: 15.195374\n",
      "res-3-iteration-40: 14.763479\n",
      "res-3-iteration-50: 14.492970\n",
      "save image: res-3-result-50.jpg\n",
      "res-3-iteration-60: 14.308968\n",
      "res-3-iteration-70: 14.170503\n",
      "res-3-iteration-80: 14.070270\n",
      "res-3-iteration-90: 13.981964\n",
      "res-3-iteration-100: 13.913269\n",
      "save image: res-3-result-100.jpg\n",
      "res-3-iteration-110: 13.856187\n",
      "res-3-iteration-120: 13.809785\n",
      "res-3-iteration-130: 13.771515\n",
      "res-3-iteration-140: 13.741038\n",
      "res-3-iteration-150: 13.719706\n",
      "save image: res-3-result-150.jpg\n",
      "res-3-iteration-160: 13.692719\n",
      "res-3-iteration-170: 13.668753\n",
      "res-3-iteration-180: 13.651217\n",
      "res-3-iteration-190: 13.632586\n",
      "res-3-iteration-200: 13.617582\n",
      "save image: res-3-result-200.jpg\n",
      "模型训练总时长为: 08:45:19\n"
     ]
    }
   ],
   "source": [
    "def main(content_path,style_path,max_iter,sample_step,content_weight,style_weight,tv_weight,num_res,gpu_chunck_size,mrf_style_stride,mrf_synthesis_stride):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \"转换与去规范化转换\"\n",
    "    # 预训练的vgg网络，其图片的均值为[0.485, 0.456, 0.406]，方差为[0.229, 0.224, 0.225].\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))])\n",
    "    denorm_transform = transforms.Normalize(mean=(-2.12, -2.04, -1.80), std=(4.37, 4.46, 4.44))\n",
    "\n",
    "    \"读取图片\"\n",
    "    if not os.path.exists(content_path):\n",
    "        raise ValueError('file %s does not exist.' % content_path)\n",
    "    if not os.path.exists(style_path):\n",
    "        raise ValueError('file %s does not exist.' % style_path)\n",
    "    content_image = cv2.imread(content_path)\n",
    "    content_image = cv2.cvtColor(content_image, cv2.COLOR_BGR2RGB)\n",
    "    content_image = transform(content_image).unsqueeze(0).to(device)\n",
    "\n",
    "    style_image = cv2.imread(style_path)\n",
    "    style_image = cv2.cvtColor(style_image, cv2.COLOR_BGR2RGB)\n",
    "    style_image = transform(style_image).unsqueeze(0).to(device)\n",
    "\n",
    "    \"重新设置图片的大小\"\n",
    "    pyramid_content_image = []\n",
    "    pyramid_style_image = []\n",
    "    for i in range(num_res):\n",
    "        content = functional.interpolate(content_image, scale_factor=1/pow(2, num_res-1-i), mode='bilinear')\n",
    "\n",
    "        style = functional.interpolate(style_image, scale_factor=1/pow(2, num_res-1-i), mode='bilinear')\n",
    "\n",
    "        pyramid_content_image.append(content)\n",
    "        pyramid_style_image.append(style)\n",
    "    \"开始训练\"\n",
    "    global iter\n",
    "    iter = 0\n",
    "    synthesis = None\n",
    "    # 创建卷积马尔可夫随机阵模型\n",
    "    cnnmrf = CNNMRF(style_image=pyramid_style_image[0], content_image=pyramid_content_image[0], device=device,\n",
    "                    content_weight=content_weight, style_weight=style_weight, tv_weight=tv_weight,\n",
    "                    gpu_chunck_size=gpu_chunck_size, mrf_synthesis_stride=mrf_synthesis_stride,\n",
    "                    mrf_style_stride=mrf_style_stride).to(device)\n",
    "\n",
    "    # 设置模块进入训练模式\n",
    "    cnnmrf.train()\n",
    "    for i in range(0, num_res):\n",
    "        # synthesis = torch.rand_like(content_image, requires_grad=True)\n",
    "        if i == 0:\n",
    "            # 低维度用于合成图像中内容图像特征提取\n",
    "            synthesis = pyramid_content_image[0].clone().requires_grad_(True).to(device)\n",
    "        else:\n",
    "            # 高维度将合成图像从更高层进行下采样\n",
    "            synthesis = unsample_synthesis(pyramid_content_image[i].shape[2], pyramid_content_image[i].shape[3], synthesis, device)\n",
    "            cnnmrf.update_style_and_content_image(style_image=pyramid_style_image[i], content_image=pyramid_content_image[i])\n",
    "        # 设置优化迭代的最大次数\n",
    "        optimizer = optim.LBFGS([synthesis], lr=1, max_iter=max_iter)\n",
    "        \"--------------------\"\n",
    "\n",
    "        def closure():\n",
    "            global iter\n",
    "            optimizer.zero_grad()\n",
    "            loss = cnnmrf(synthesis)\n",
    "            loss.backward(retain_graph=True)\n",
    "            # 显示出每次优化的损失值\n",
    "            if (iter + 1) % 10 == 0:\n",
    "                print('res-%d-iteration-%d: %f' % (i+1, iter + 1, loss.item()))\n",
    "            # 保存图片\n",
    "            if (iter + 1) % sample_step == 0 or iter + 1 == max_iter:\n",
    "                image = get_synthesis_image(synthesis, denorm_transform, device)\n",
    "                image = functional.interpolate(image.unsqueeze(0), size=content_image.shape[2:4], mode='bilinear')\n",
    "                torchvision.utils.save_image(image.squeeze(), './Li_Style_Transfer_results/results/res-%d-result-%d.jpg' % (i+1, iter + 1))\n",
    "                print('save image: res-%d-result-%d.jpg' % (i+1, iter + 1))\n",
    "            iter += 1\n",
    "            if iter == max_iter:\n",
    "                iter = 0\n",
    "            return loss\n",
    "\n",
    "        \"----------------------\"\n",
    "        optimizer.step(closure)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    mylist = os.listdir('./Test_dataset')\n",
    "    root_path = './Test_dataset/'\n",
    "\n",
    "#     random.seed(1)\n",
    "#     content_image_path = root_path + random.choice(mylist) \n",
    "    content_image_path = root_path + '000000002897.jpg'\n",
    "    style_image_path = './Alexnet_Style_Transfer_results/style_input/Kazimir.jpg'  #指定一个风格图片\n",
    "\n",
    "    start_time = time.time()\n",
    "    main(content_image_path,       #内容图片的路径\n",
    "         style_image_path,    #风格图片的路径\n",
    "             200,        #最大迭代次数\n",
    "             50,         #每多少步采样\n",
    "             1,          #内容图像损失函数权重\n",
    "             0.4,        #风格图像损失函数权重\n",
    "             0.1,        #总变差损失函数权重\n",
    "             3,          #循环次数\n",
    "             512,        #gpu块大小\n",
    "             2,          #马尔科夫随机阵风格步长\n",
    "             2)          #马尔科夫随机阵合成步长\n",
    "    end_time = time.time()\n",
    "    elapsed_time = time.strftime(\"%H:%M:%S\", time.gmtime(end_time - start_time))\n",
    "\n",
    "    print(\"模型训练总时长为: %s\" % elapsed_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "96c271ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2  \n",
    "import matplotlib.pyplot as plt\n",
    "img = cv2.imread('./Li_Style_Transfer_results/results/Francis/res-3-result-200.jpg')\n",
    "# b,g,r = cv2.split(img) \n",
    "# img_rgb = cv2.merge([r,g,b]) \n",
    "# plt.figure() \n",
    "# plt.imshow(img_rgb) \n",
    "# plt.show() \n",
    "cv2.imwrite('./4.jpg',cv2.cvtColor(img,cv2.COLOR_BGR2RGB))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f7b978",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
